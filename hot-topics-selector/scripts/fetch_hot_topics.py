#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è´¢ç»çƒ­ç‚¹æŠ“å–å·¥å…·
åŸºäº TrendRadar é¡¹ç›®èƒ½åŠ›ï¼ŒæŠ“å–çœŸå®çƒ­ç‚¹èµ„è®¯
"""

import json
import time
import random
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union

import requests
import pytz


class HotTopicsFetcher:
    """çƒ­ç‚¹æŠ“å–å™¨"""
    
    # API åŸºç¡€åœ°å€
    API_BASE_URL = "https://newsnow.busiyi.world/api/s"
    
    # æ”¯æŒçš„å¹³å°
    PLATFORMS = {
        "cls-hot": "è´¢è”ç¤¾çƒ­é—¨",
        "_36kr": "36æ°ª",
        "gelonghui": "æ ¼éš†æ±‡",
        "toutiao": "ä»Šæ—¥å¤´æ¡",
        "baidu": "ç™¾åº¦çƒ­æœ",
        "weibo": "å¾®åš",
        "douyin": "æŠ–éŸ³",
        "zhihu": "çŸ¥ä¹",
    }
    
    # è´¢ç»ç›¸å…³å¹³å°ï¼ˆä¸“ä¸šï¼‰
    FINANCE_PLATFORMS = ["cls-hot", "_36kr", "gelonghui"]
    
    # ç»¼åˆèµ„è®¯å¹³å°ï¼ˆå¤§ä¼—å…³æ³¨åº¦é«˜ï¼‰
    GENERAL_PLATFORMS = ["toutiao", "baidu", "weibo", "douyin", "zhihu"]
    
    # é»˜è®¤æ¨èå¹³å°ï¼ˆå¹³è¡¡ä¸“ä¸šå’Œå¤§ä¼—ï¼‰
    RECOMMENDED_PLATFORMS = ["weibo", "baidu", "toutiao", "cls-hot", "douyin"]
    
    def __init__(self, proxy_url: Optional[str] = None):
        self.proxy_url = proxy_url
        self.session = requests.Session()
        
    def fetch_platform_data(
        self, 
        platform_id: str, 
        max_retries: int = 2,
        retry_wait: int = 3
    ) -> Tuple[Optional[Dict], str]:
        """
        æŠ“å–æŒ‡å®šå¹³å°çš„çƒ­ç‚¹æ•°æ®
        
        Args:
            platform_id: å¹³å°IDï¼ˆå¦‚ cls-hotï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_wait: é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
            (æ•°æ®å­—å…¸, å¹³å°åç§°)
        """
        platform_name = self.PLATFORMS.get(platform_id, platform_id)
        url = f"{self.API_BASE_URL}?id={platform_id}&latest"
        
        # é…ç½®ä»£ç†
        proxies = None
        if self.proxy_url:
            proxies = {"http": self.proxy_url, "https": self.proxy_url}
        
        # è¯·æ±‚å¤´
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Cache-Control": "no-cache",
        }
        
        # é‡è¯•é€»è¾‘
        for attempt in range(max_retries + 1):
            try:
                response = self.session.get(url, proxies=proxies, headers=headers, timeout=10)
                response.raise_for_status()
                
                data = json.loads(response.text)
                return data, platform_name
                
            except Exception as e:
                if attempt < max_retries:
                    wait_time = retry_wait + random.uniform(1, 2) * attempt
                    print(f"âš ï¸ {platform_name} æŠ“å–å¤±è´¥: {e}ï¼Œ{wait_time:.1f}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print(f"âŒ {platform_name} æŠ“å–å¤±è´¥: {e}")
                    return None, platform_name
        
        return None, platform_name
    
    def fetch_all_platforms(
        self,
        platforms: Optional[List[str]] = None,
        request_interval: int = 1000
    ) -> Dict:
        """
        æŠ“å–å¤šä¸ªå¹³å°çš„çƒ­ç‚¹æ•°æ®
        
        Args:
            platforms: å¹³å°åˆ—è¡¨ï¼ŒNone è¡¨ç¤ºä½¿ç”¨è´¢ç»å¹³å°
            request_interval: è¯·æ±‚é—´éš”ï¼ˆæ¯«ç§’ï¼‰
        
        Returns:
            åŒ…å«æ‰€æœ‰å¹³å°æ•°æ®çš„å­—å…¸
        """
        if platforms is None:
            platforms = self.FINANCE_PLATFORMS
        
        results = {}
        
        print(f"\nğŸ” å¼€å§‹æŠ“å– {len(platforms)} ä¸ªå¹³å°çš„çƒ­ç‚¹...")
        print(f"å¹³å°åˆ—è¡¨: {', '.join([self.PLATFORMS.get(p, p) for p in platforms])}")
        
        for i, platform_id in enumerate(platforms):
            data, platform_name = self.fetch_platform_data(platform_id)
            
            if data and "items" in data:
                results[platform_id] = {
                    "name": platform_name,
                    "items": data["items"],
                    "count": len(data["items"])
                }
                print(f"âœ… {platform_name}: {len(data['items'])} æ¡")
            else:
                results[platform_id] = {
                    "name": platform_name,
                    "items": [],
                    "count": 0
                }
                print(f"âŒ {platform_name}: 0 æ¡")
            
            # è¯·æ±‚é—´éš”
            if i < len(platforms) - 1:
                interval = request_interval / 1000 + random.uniform(-0.1, 0.1)
                time.sleep(max(0.5, interval))
        
        return results
    
    def filter_by_keywords(
        self,
        data: Dict,
        include_keywords: Optional[List[str]] = None,
        exclude_keywords: Optional[List[str]] = None
    ) -> Dict:
        """
        æ ¹æ®å…³é”®è¯è¿‡æ»¤çƒ­ç‚¹
        
        Args:
            data: åŸå§‹æ•°æ®
            include_keywords: åŒ…å«å…³é”®è¯åˆ—è¡¨
            exclude_keywords: æ’é™¤å…³é”®è¯åˆ—è¡¨
        
        Returns:
            è¿‡æ»¤åçš„æ•°æ®
        """
        if not include_keywords and not exclude_keywords:
            return data
        
        filtered_data = {}
        
        for platform_id, platform_data in data.items():
            filtered_items = []
            
            for item in platform_data.get("items", []):
                title = item.get("title", "")
                
                # æ£€æŸ¥æ’é™¤å…³é”®è¯
                if exclude_keywords:
                    if any(kw.lower() in title.lower() for kw in exclude_keywords):
                        continue
                
                # æ£€æŸ¥åŒ…å«å…³é”®è¯
                if include_keywords:
                    if not any(kw.lower() in title.lower() for kw in include_keywords):
                        continue
                
                filtered_items.append(item)
            
            filtered_data[platform_id] = {
                **platform_data,
                "items": filtered_items,
                "count": len(filtered_items)
            }
        
        return filtered_data
    
    def save_to_file(
        self,
        data: Dict,
        output_path: Optional[str] = None,
        filename_prefix: str = "hot_topics"
    ) -> str:
        """
        ä¿å­˜æ•°æ®åˆ° JSON æ–‡ä»¶
        
        Args:
            data: çƒ­ç‚¹æ•°æ®
            output_path: è¾“å‡ºè·¯å¾„
            filename_prefix: æ–‡ä»¶åå‰ç¼€
        
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if output_path is None:
            output_path = "/tmp"
        
        Path(output_path).mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now(pytz.timezone("Asia/Shanghai")).strftime("%Y-%m-%d_%H%M%S")
        filename = f"{filename_prefix}_{timestamp}.json"
        filepath = Path(output_path) / filename
        
        # æ·»åŠ å…ƒæ•°æ®
        output_data = {
            "fetch_time": datetime.now(pytz.timezone("Asia/Shanghai")).isoformat(),
            "platforms_count": len(data),
            "total_items": sum(p.get("count", 0) for p in data.values()),
            "data": data
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜: {filepath}")
        return str(filepath)
    
    def print_summary(self, data: Dict):
        """æ‰“å°æ•°æ®æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š çƒ­ç‚¹æ•°æ®æ‘˜è¦")
        print("="*60)
        
        total_items = sum(p.get("count", 0) for p in data.values())
        print(f"æ€»å¹³å°æ•°: {len(data)}")
        print(f"æ€»çƒ­ç‚¹æ•°: {total_items}")
        
        print("\nå„å¹³å°æ•°æ®:")
        for platform_id, platform_data in data.items():
            name = platform_data.get("name", platform_id)
            count = platform_data.get("count", 0)
            print(f"  â€¢ {name}: {count} æ¡")
        
        # æ˜¾ç¤º Top 5
        print("\nğŸ”¥ ç»¼åˆçƒ­åº¦ Top 5:")
        all_items = []
        for platform_id, platform_data in data.items():
            for idx, item in enumerate(platform_data.get("items", [])[:10], 1):
                all_items.append({
                    "title": item.get("title", ""),
                    "rank": idx,
                    "platform": platform_data.get("name", ""),
                    "url": item.get("url", ""),
                })
        
        # æŒ‰æ’åæ’åº
        all_items.sort(key=lambda x: x["rank"])
        for i, item in enumerate(all_items[:5], 1):
            print(f"{i}. {item['title']}")
            print(f"   æ¥æº: {item['platform']} | æ’å: #{item['rank']}")
        
        print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è´¢ç»çƒ­ç‚¹æŠ“å–å·¥å…·')
    parser.add_argument('--platforms', type=str, help='å¹³å°åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--keywords', type=str, help='åŒ…å«å…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--exclude', type=str, help='æ’é™¤å…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--output', type=str, default='/tmp', help='è¾“å‡ºè·¯å¾„')
    parser.add_argument('--finance', action='store_true', help='åªæŠ“å–è´¢ç»å¹³å°')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æŠ“å–å™¨
    fetcher = HotTopicsFetcher()
    
    # ç¡®å®šå¹³å°åˆ—è¡¨
    if args.platforms:
        platforms = [p.strip() for p in args.platforms.split(',')]
    elif args.finance:
        platforms = HotTopicsFetcher.FINANCE_PLATFORMS
    else:
        # é»˜è®¤ä½¿ç”¨æ¨èå¹³å°ï¼ˆå¹³è¡¡ä¸“ä¸šå’Œå¤§ä¼—ï¼‰
        platforms = HotTopicsFetcher.RECOMMENDED_PLATFORMS
    
    # æŠ“å–æ•°æ®
    data = fetcher.fetch_all_platforms(platforms)
    
    # å…³é”®è¯è¿‡æ»¤
    include_keywords = [k.strip() for k in args.keywords.split(',')] if args.keywords else None
    exclude_keywords = [k.strip() for k in args.exclude.split(',')] if args.exclude else None
    
    if include_keywords or exclude_keywords:
        print("\nğŸ” åº”ç”¨å…³é”®è¯è¿‡æ»¤...")
        data = fetcher.filter_by_keywords(data, include_keywords, exclude_keywords)
    
    # ä¿å­˜æ•°æ®
    filepath = fetcher.save_to_file(data, args.output)
    
    # æ‰“å°æ‘˜è¦
    fetcher.print_summary(data)
    
    return filepath


if __name__ == "__main__":
    main()
